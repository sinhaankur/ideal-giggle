# ğŸ¤– AI Companion - On-Device AI Setup

This is a privacy-first AI companion that runs entirely on your device using **Ollama** for local LLM inference. No data leaves your machine.

## ğŸ¯ Features

- âœ… **On-Device AI** - All processing happens locally
- âœ… **Privacy-First** - Zero cloud connectivity required
- âœ… **Emotion Detection** - Real-time facial expression analysis
- âœ… **Continuous Voice** - Always-listening voice commands
- âœ… **Multi-Companion** - Create multiple AI companions
- âœ… **Persistent Memory** - Relationships and memories saved locally
- âœ… **Calendar Integration** - Sync with Google, Outlook, Apple Calendar
- âœ… **Zero Latency** - Local processing means instant responses

## ğŸš€ Quick Start

### Option 1: Automatic Startup (Recommended)

```bash
cd /home/ankursinha/building-management-ai
./start_ai_companion.sh
```

This script will:
1. Set up the Python environment
2. Start Ollama container (if Docker/Podman available)
3. Launch the Flask app
4. Open the companion interface

### Option 2: Manual Startup

#### 1. Start Ollama (On-Device LLM)
```bash
# Using Docker (recommended)
docker run -d --name ollama -p 11434:11434 -v ollama:/root/.ollama ollama/ollama

# Or using Podman
podman run -d --name ollama -p 11434:11434 -v ollama:/root/.ollama ollama/ollama
```

#### 2. Pull a Model (Optional - downloads on first use)
```bash
docker exec ollama ollama pull neural-chat
# or
docker exec ollama ollama pull mistral
```

#### 3. Start Flask App
```bash
cd /home/ankursinha/building-management-ai
source venv/bin/activate
python3 app.py
```

#### 4. Open in Browser
```
http://localhost:5000
```

## ğŸ› ï¸ System Requirements

- **CPU**: 2+ cores recommended
- **RAM**: 4GB minimum, 8GB+ recommended (depends on model)
- **Disk**: 5GB+ for Ollama models
- **Python**: 3.8+
- **Docker** or **Podman** (for containerized Ollama)

## ğŸ“Š API Endpoints

### On-Device AI Status
```bash
curl http://localhost:5000/api/ai/status
```

Response shows:
- Flask app status
- Ollama (local LLM) status
- Available models
- Feature status

### Health Check
```bash
curl http://localhost:5000/api/ai/health
```

### Available Models
```bash
curl http://localhost:5000/api/ai/models
```

### Chat with Companion
```bash
curl -X POST http://localhost:5000/api/companion/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user_123",
    "companion_id": "anita",
    "message": "Hello!"
  }'
```

## ğŸ§  Supported Ollama Models

Popular lightweight models for on-device use:

- **neural-chat** (7B) - Balanced, recommended
- **mistral** (7B) - Fast, good quality
- **llama2** (7B-13B) - Reliable, well-tested
- **phi** (2.7B) - Smallest, fastest
- **orca-mini** (3B) - Compact alternative

Pull a model:
```bash
docker exec ollama ollama pull neural-chat
```

Check available models:
```bash
curl http://localhost:11434/api/tags
```

## ğŸ” Privacy & Security

- **No Cloud Calls**: All inference happens locally
- **No API Keys**: No OpenAI, Anthropic, or other API keys needed
- **Local Storage**: Relationships and memories stored in `.companions/` directory
- **Device Only**: Your data never leaves your machine
- **Open Source**: Full control over your AI

## ğŸ¤ Voice Input

- Click the **ğŸ¤ Mic** button to toggle continuous listening
- Speak naturally - phrases are appended together
- Button pulses when actively listening
- Speech recognition runs in browser (Web Speech API)

## ğŸ‘¥ Multi-Companion Support

Create multiple AI companions with different personalities:

1. Click **â• New** to create a companion
2. Set name and personality traits:
   - Warmth (0-100)
   - Humor (0-100)
   - Intelligence (0-100)
   - Mystery (0-100)
   - Ambition (0-100)
3. Start chatting!

## ğŸ“… Calendar Sync

Companions can be aware of your schedule:

1. Click **ğŸ“… Calendar**
2. Select provider (Google, Outlook, Apple, iCal)
3. Configure privacy settings
4. Companion will reference your events

## ğŸ› Troubleshooting

### Ollama not starting
```bash
# Check Docker
docker ps

# Check if port 11434 is in use
lsof -i :11434

# Clean up old containers
docker rm -f ollama
```

### Flask app not responding
```bash
# Check logs
tail -f app.log

# Kill hanging processes
pkill -9 -f "python.*app.py"

# Restart
python3 app.py
```

### Slow responses
- LLM response time depends on model size and CPU
- Smaller models (phi, orca-mini) are faster
- More RAM = faster processing
- GPU acceleration can be enabled with compatible hardware

### Voice recognition not working
- Check browser permissions for microphone
- Works best in Chrome, Edge, Safari
- Firefox has limited support
- Requires HTTPS in production (HTTP OK for localhost)

## ğŸ“š Architecture

```
Web Browser (localhost:5000)
    â†“
Flask API (Python)
    â†“
AI Handler (Unified interface)
    â†“
Ollama Client â†’ Ollama Container â†’ Local LLM Model
```

All communication is local - no internet required.

## ğŸ”„ Data Flow (All Local)

1. User types/speaks message
2. Browser sends to Flask API
3. Flask enriches with context (emotion, companion personality)
4. Unified AI Handler routes to Ollama
5. Ollama generates response locally
6. Response sent back to browser
7. Relationship & memories updated locally

## ğŸ“– Configuration

Environment variables (optional):
```bash
export OLLAMA_URL=http://localhost:11434
export FLASK_DEBUG=True
export PORT=5000
```

## ğŸš¦ Status Indicators

- ğŸŸ¢ **Green**: Full on-device AI working
- ğŸŸ¡ **Yellow**: Fallback mode (Ollama unavailable)
- ğŸ”´ **Red**: Service error

Check status at: http://localhost:5000/api/ai/status

## ğŸ“ Logs

```bash
# Flask app logs
tail -f app.log

# Ollama logs
docker logs ollama -f
```

## ğŸ“ Learning & Customization

Companions learn from interactions:
- Relationship levels tracked
- Emotional context stored
- Memory of past conversations
- Personality influences responses

Customize companion personalities by editing traits.

## ğŸ¤ Integration

### Pull from Other Apps
The Flask API is fully RESTful - integrate with:
- Discord bots
- Telegram bots
- Custom applications
- Smart home systems

All endpoints are at `/api/companion/*` and `/api/ai/*`

## ğŸ“¦ Files Structure

```
.
â”œâ”€â”€ app.py                    # Flask entry point
â”œâ”€â”€ start_ai_companion.sh     # Startup script (this file)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ ai_client.py      # Unified AI interface
â”‚   â”‚   â”œâ”€â”€ ollama_client.py  # Ollama integration
â”‚   â”‚   â”œâ”€â”€ companion.py      # Companion personality
â”‚   â”‚   â””â”€â”€ emotion_*.py      # Emotion detection
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py         # Core API routes
â”‚   â”‚   â””â”€â”€ companion_routes.py # Companion endpoints
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ companion_memory.py # Memory management
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ db.py             # Local database
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ companion_app.html # Web UI
â””â”€â”€ .companions/              # Saved companions & memories
```

## ğŸ¯ Next Steps

1. **First Run**: Open http://localhost:5000
2. **Create Companion**: Click â• New
3. **Start Chatting**: Begin your conversation
4. **Enable Features**: Turn on voice, emotion, calendar as needed
5. **Customize**: Edit traits to match your preferences

## ğŸ’¬ Support

Check logs for detailed error information:
```bash
curl http://localhost:5000/api/ai/status | python3 -m json.tool
```

---

**Privacy First, AI Everywhere** ğŸ”’ğŸ¤–
